{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JN2l2mplb4wq",
        "outputId": "8ac425a4-61b9-4911-bedc-4162c88bac83"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-1-25deaaebb2dd>:16: DeprecationWarning: Please use `rotate` from the `scipy.ndimage` namespace, the `scipy.ndimage.interpolation` namespace is deprecated.\n",
            "  from scipy.ndimage.interpolation import rotate\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "import os\n",
        "import pandas as pd\n",
        "import cv2 as cv\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "import torch\n",
        "\n",
        "import random\n",
        "\n",
        "from tqdm import tqdm\n",
        "\n",
        "\n",
        "from numpy import random\n",
        "\n",
        "from scipy.ndimage.interpolation import rotate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WhRGRk9z2E87"
      },
      "outputs": [],
      "source": [
        "config = dict()\n",
        "config['metadata_dir'] = '/content/drive/My Drive/Dataset/HAM10000/HAM10000_metadata.csv'\n",
        "config['image_data_dir'] = '/content/drive/My Drive/Dataset/image_frame'\n",
        "config['save_data_dir'] = '/content/drive/My Drive/Dataset/save_data/'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mu_fHaj8wX5F",
        "outputId": "973de7bd-1c6a-4177-fa72-e6d2a0cdc630"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "{'bkl': 10.963554667998004, 'nv': 66.93959061407888, 'df': 1.1382925611582626, 'mel': 11.10334498252621, 'vasc': 1.4078881677483774, 'bcc': 5.122316525212182, 'akiec': 3.2551173240139795}\n"
          ]
        }
      ],
      "source": [
        "# Acces to data \n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Load the CSV files for training and validation\n",
        "metadata = pd.read_csv(config['metadata_dir'])\n",
        "\n",
        "raw_lesion_name = metadata['dx']\n",
        "\n",
        "all = dict()\n",
        "for element in raw_lesion_name:\n",
        "  if element not in all:\n",
        "    all[element] = 0\n",
        "  else:\n",
        "    all[element] += 1\n",
        "\n",
        "for k in all:\n",
        "  all[k] = float((all[k] / 10015) * 100)\n",
        "\n",
        "print(all)\n",
        "\n",
        "\n",
        "\n",
        "# the code is totally not use here ............\n",
        "# Add the .jpg file extension to the filenames in the dataframe\n",
        "metadata['image_id'] = metadata['image_id'].apply(lambda x: f\"{x}.jpg\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tEMbXEEc9vjK"
      },
      "outputs": [],
      "source": [
        "class DataProcessor:\n",
        "\n",
        "  def __init__(self, image_frame_dir, meta_data_dir, random_rotate):\n",
        "\n",
        "    # Dir of Image Frame and Metadata:\n",
        "    self.image_frame_dir = image_frame_dir\n",
        "    self.meta_data_dir = meta_data_dir\n",
        "\n",
        "    # Training Image Data: NT * H * W * C    \n",
        "    self.training_img_data = None  # a list of image idx\n",
        "    self.train_diag_result = []\n",
        "\n",
        "    # Training Image Data: NV * H * W * C  \n",
        "    self.validation_img_data = None  # a list of image idx\n",
        "    self.valid_diag_result = []\n",
        "\n",
        "    # geometry feature\n",
        "    self.rotate = random_rotate\n",
        "    self._temp_imgID_leision_finder = self._label_image_matcher()\n",
        "    \n",
        "\n",
        "\n",
        "  def _label_transformer(self):\n",
        "    \"\"\"\n",
        "    Transform lesion name (bkl ...) to a integer.\n",
        "    \"\"\"\n",
        "    train_metadata = pd.read_csv(self.meta_data_dir)\n",
        "    raw_lesion_name = train_metadata['dx']\n",
        "\n",
        "    processed_name = dict()\n",
        "\n",
        "    count = 0\n",
        "    for lesion in raw_lesion_name:\n",
        "      if lesion not in processed_name:\n",
        "        processed_name[lesion] = count\n",
        "        count += 1\n",
        "\n",
        "    return processed_name\n",
        "\n",
        "\n",
        "  def _label_image_matcher(self):\n",
        "    \"\"\"\n",
        "    Match each image with its label.\n",
        "\n",
        "    {'img_011': 0, 'img_012': 2, .....}\n",
        "    \"\"\"\n",
        "    train_metadata = pd.read_csv(self.meta_data_dir)\n",
        "    image_ids = list(train_metadata['image_id'])\n",
        "    lesion_ids = list(train_metadata['dx'])\n",
        "\n",
        "    assert len(image_ids) == len(lesion_ids)\n",
        "\n",
        "    finder = dict()\n",
        "    self.lebal_info = self._label_transformer()\n",
        "\n",
        "    for i in range(len(image_ids)):\n",
        "      if image_ids[i] in finder:\n",
        "        raise Exception('Duplicate Image Exists !')\n",
        "      finder[image_ids[i]] = self.lebal_info[lesion_ids[i]]\n",
        "\n",
        "    return finder\n",
        "\n",
        "\n",
        "  \n",
        "  def _split_train_valid(self, train_portion):\n",
        "    \"\"\"\n",
        "    Split the data into: \n",
        "      - train_portion% of training\n",
        "      - (1-train_portion) of validation\n",
        "    \"\"\"\n",
        "    assert train_portion < 1\n",
        "\n",
        "    temp_imgID_leision_finder = self._label_image_matcher()\n",
        "    \n",
        "    full_info = {0: [], 1: [], 2:[], 3: [], 4: [], 5: [], 6: []}\n",
        "\n",
        "    train_image_idx = []\n",
        "    valid_image_idx = []\n",
        "\n",
        "    for image_id in temp_imgID_leision_finder:\n",
        "\n",
        "      lesion = temp_imgID_leision_finder[image_id]\n",
        "      \n",
        "      full_info[lesion].append(image_id)\n",
        "\n",
        "\n",
        "    for lesion in full_info:\n",
        "\n",
        "      lesion_images = full_info[lesion]\n",
        "\n",
        "      full_len = len(lesion_images)\n",
        "      train_len = int(np.floor(train_portion * full_len))\n",
        "\n",
        "      train_image_idx += lesion_images[0:train_len + 1]\n",
        "      valid_image_idx += lesion_images[train_len + 1:]\n",
        "\n",
        "      assert len(lesion_images[0:train_len + 1]) + \\\n",
        "      len(lesion_images[train_len + 1:]) == full_len\n",
        "\n",
        "    random.shuffle(train_image_idx)\n",
        "    random.shuffle(valid_image_idx)\n",
        "\n",
        "    return train_image_idx, valid_image_idx\n",
        "\n",
        "\n",
        "\n",
        "  def _read_image(self, train_portion):\n",
        "  \n",
        "    train_img_idx, valid_image_idx = self._split_train_valid(train_portion)\n",
        "\n",
        "    curr_dir = self.image_frame_dir\n",
        "    \n",
        "    print('..... Processing Training Image and Label .....')\n",
        "    for img_dir in tqdm(train_img_idx):\n",
        "\n",
        "      full_curr_dir = curr_dir + '/' + img_dir + '.jpg'\n",
        "\n",
        "      image = cv.imread(full_curr_dir)\n",
        "      image = cv.cvtColor(image, cv.COLOR_BGR2RGB)\n",
        "\n",
        "      # generate random angle between 0 and 359\n",
        "      random_angle = random.randint(0, 360)\n",
        "      rotated_image = rotate(image, random_angle)\n",
        "\n",
        "      resized_rotate = np.expand_dims(cv.resize(rotated_image, dsize=(299, 299)), axis=0)\n",
        "      resized_original = np.expand_dims(cv.resize(image, dsize=(299, 299)), axis=0)\n",
        "\n",
        "      img_id = img_dir.split('.')[0]\n",
        "      temp_result = self._temp_imgID_leision_finder[img_id]\n",
        "      self.train_diag_result.append(temp_result)\n",
        "      self.train_diag_result.append(temp_result)\n",
        "\n",
        "      if self.training_img_data is None:\n",
        "          self.training_img_data = resized_original\n",
        "          self.training_img_data = np.concatenate((self.training_img_data, resized_rotate), axis=0)\n",
        "      else:\n",
        "        self.training_img_data = \\\n",
        "        np.concatenate((self.training_img_data, resized_original), axis=0)\n",
        "        self.training_img_data = np.concatenate((self.training_img_data, resized_rotate), axis=0)\n",
        "\n",
        "\n",
        "    print('..... Processing Validation Image and Label .....')\n",
        "    for img_dir in tqdm(valid_image_idx):\n",
        "\n",
        "      full_curr_dir = curr_dir + '/' + img_dir + '.jpg'\n",
        "      \n",
        "      image = cv.imread(full_curr_dir)\n",
        "      image = cv.cvtColor(image, cv.COLOR_BGR2RGB)\n",
        "\n",
        "      # random_angle = random.randint(0, 360)\n",
        "      # rotated_image = rotate(image, random_angle)\n",
        "\n",
        "\n",
        "      resized_rotate = np.expand_dims(cv.resize(rotated_image, dsize=(299, 299)), axis=0)\n",
        "      # resized_original = np.expand_dims(cv.resize(image, dsize=(299, 299)), axis=0)\n",
        "\n",
        "      img_id = img_dir.split('.')[0]\n",
        "      temp_result = self._temp_imgID_leision_finder[img_id]\n",
        "      self.valid_diag_result.append(temp_result)\n",
        "      # self.valid_diag_result.append(temp_result)\n",
        "\n",
        "      if self.validation_img_data is None:\n",
        "          self.validation_img_data = resized_original\n",
        "        #   self.validation_img_data = \\\n",
        "        # np.concatenate((self.validation_img_data, resized_rotate), axis=0)\n",
        "\n",
        "      else:\n",
        "        self.validation_img_data = \\\n",
        "        np.concatenate((self.validation_img_data, resized_original), axis=0)\n",
        "        # self.validation_img_data = \\\n",
        "        # np.concatenate((self.validation_img_data, resized_rotate), axis=0)\n",
        "\n",
        "    print('.....Image Processing is Completed......')\n",
        "\n",
        "\n",
        "  def start_processing(self):\n",
        "    \"\"\"\n",
        "    The Final Function to run ....\n",
        "    \"\"\"\n",
        "\n",
        "    self._read_image(0.7)\n",
        "\n",
        "    print('== Finished Image Reading ==')\n",
        "\n",
        "    assert len(self.training_img_data) == len(self.train_diag_result)\n",
        "    assert len(self.validation_img_data) == len(self.valid_diag_result)\n",
        "\n",
        "    print('Start saving data ....')\n",
        "    train_data = self.training_img_data\n",
        "    print('Finish Extract Data')\n",
        "    train_label = np.array(self.train_diag_result)\n",
        "    print('Finish Extract Label')\n",
        "\n",
        "    shuffler = np.random.permutation(len(train_data))\n",
        "    train_data = train_data[shuffler]\n",
        "    train_label = train_label[shuffler]\n",
        "    print('Finish Shuffle Data')\n",
        "\n",
        "\n",
        "    print('Start saving data ....')\n",
        "    valid_data = self.validation_img_data\n",
        "    print('Finish Extract Data')\n",
        "    valid_label = np.array(self.valid_diag_result)\n",
        "    print('Finish Extract Label')\n",
        "\n",
        "    shuffler = np.random.permutation(len(valid_data))\n",
        "    valid_data = valid_data[shuffler]\n",
        "    valid_label = valid_label[shuffler]\n",
        "    print('Finish Shuffle Data')\n",
        "    \n",
        "    print('Connecting to Drive ....')\n",
        "    #drive.mount('/content/drive')\n",
        "    print('Connected to Drive ....')\n",
        "\n",
        "    with open(config['save_data_dir'] + 'augfull_train_data.npy', 'wb') as td:\n",
        "      print('starting saving training ....')\n",
        "      with tqdm(total=train_data.nbytes, unit='B', unit_scale=True, desc=config['save_data_dir'] + 'augfull_train_data.npy') as pbar:\n",
        "        np.save(td, train_data)\n",
        "        pbar.update(train_data.nbytes)\n",
        "\n",
        "    with open(config['save_data_dir'] + 'augfull_train_label.npy', 'wb') as tl:\n",
        "      with tqdm(total=train_label.nbytes, unit='B', unit_scale=True, desc=config['save_data_dir'] + 'augfull_train_label.npy') as pbar:\n",
        "        np.save(tl, train_label)\n",
        "        pbar.update(train_label.nbytes)\n",
        "\n",
        "    with open(config['save_data_dir'] + 'augfull_valid_data.npy', 'wb') as vd:\n",
        "      with tqdm(total=valid_data.nbytes, unit='B', unit_scale=True, desc=config['save_data_dir'] + 'augfull_valid_data.npy') as pbar:\n",
        "        np.save(vd, valid_data)\n",
        "        pbar.update(valid_data.nbytes)\n",
        "\n",
        "    with open(config['save_data_dir'] + 'augfull_valid_label.npy', 'wb') as vl:\n",
        "      with tqdm(total=valid_label.nbytes, unit='B', unit_scale=True, desc=config['save_data_dir'] + 'augfull_valid_label.npy') as pbar:\n",
        "        np.save(vl, valid_label)\n",
        "        pbar.update(valid_label.nbytes)\n",
        "        #np.save(vl, valid_label)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "xzsCLOApN089",
        "outputId": "7fc116fb-18bf-4945-cee7-6309b2d14123"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "..... Processing Training Image and Label .....\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 7014/7014 [4:52:04<00:00,  2.50s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "..... Processing Validation Image and Label .....\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 3001/3001 [48:32<00:00,  1.03it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            ".....Image Processing is Completed......\n",
            "== Finished Image Reading ==\n",
            "Start saving data ....\n",
            "Finish Extract Data\n",
            "Finish Extract Label\n",
            "Finish Shuffle Data\n",
            "Start saving data ....\n",
            "Finish Extract Data\n",
            "Finish Extract Label\n",
            "Finish Shuffle Data\n",
            "Connecting to Drive ....\n",
            "Connected to Drive ....\n",
            "starting saving training ....\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/Dataset/save_data/augfull_train_data.npy: 100%|██████████| 3.76G/3.76G [00:15<00:00, 239MB/s]\n",
            "/content/drive/My Drive/Dataset/save_data/augfull_train_label.npy: 100%|██████████| 112k/112k [00:00<00:00, 40.4MB/s]\n",
            "/content/drive/My Drive/Dataset/save_data/augfull_valid_data.npy: 100%|██████████| 805M/805M [00:02<00:00, 306MB/s]\n",
            "/content/drive/My Drive/Dataset/save_data/augfull_valid_label.npy: 100%|██████████| 24.0k/24.0k [00:00<00:00, 7.79MB/s]\n"
          ]
        }
      ],
      "source": [
        "dts = DataProcessor(config['image_data_dir'], config['metadata_dir'], False)\n",
        "dts.start_processing()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "UkLTlEHWShz5"
      },
      "outputs": [],
      "source": [
        "def proportion_checker(training_label_dir, validation_label_dir):\n",
        "    drive.mount('/content/drive')\n",
        "\n",
        "\n",
        "    l0, l1, l2, l3, l4, l5, l6 = 0, 0, 0, 0, 0, 0, 0\n",
        "\n",
        "    train_label = np.load(training_label_dir).tolist()\n",
        "    t = len(train_label)\n",
        "\n",
        "    for label in train_label:\n",
        "      if label == 0:\n",
        "        l0 += 1\n",
        "      if label == 1:\n",
        "        l1 += 1\n",
        "      if label == 2:\n",
        "        l2 += 1\n",
        "      if label == 3:\n",
        "        l3 += 1\n",
        "      if label == 4:\n",
        "        l4 += 1\n",
        "      if label == 5:\n",
        "        l5 += 1\n",
        "      if label == 6:\n",
        "        l6 += 1\n",
        "\n",
        "\n",
        "    print('Training Set: Label0 {}%, Label1: {}%, Label2: {}%, Label3: {}%, Label4: {}%, Label5: {}, Label6: {}'\n",
        "    .format(l0*100/t, l1*100/t, l2*100/t, l3*100/t, l4*100/t, l5*100/t, l6*100/t))\n",
        "\n",
        "\n",
        "\n",
        "    l0, l1, l2, l3, l4, l5, l6 = 0, 0, 0, 0, 0, 0, 0\n",
        "\n",
        "    valid_label = np.load(validation_label_dir).tolist()\n",
        "    t = len(valid_label)\n",
        "\n",
        "    for label in valid_label:\n",
        "      if label == 0:\n",
        "        l0 += 1\n",
        "      if label == 1:\n",
        "        l1 += 1\n",
        "      if label == 2:\n",
        "        l2 += 1\n",
        "      if label == 3:\n",
        "        l3 += 1\n",
        "      if label == 4:\n",
        "        l4 += 1\n",
        "      if label == 5:\n",
        "        l5 += 1\n",
        "      if label == 6:\n",
        "        l6 += 1\n",
        "\n",
        "    \n",
        "    print('Validation Set: Label0 {}%, Label1: {}%, Label2: {}%, Label3: {}%, Label4: {}%, Label5: {}, Label6: {}'\n",
        "    .format(l0*100/t, l1*100/t, l2*100/t, l3*100/t, l4*100/t, l5*100/t, l6*100/t))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YnsODL8CTTIg"
      },
      "outputs": [],
      "source": [
        "training_label_dir = config['save_data_dir'] + 'aug1000_train_label.npy'\n",
        "validation_label_dir = config['save_data_dir'] + 'aug1000_valid_label.npy'\n",
        "\n",
        "proportion_checker(training_label_dir, validation_label_dir)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "machine_shape": "hm",
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}